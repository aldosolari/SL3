---
title: "Statistical Learning - module III"
output: markdowntemplates::minimal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Program

**Conformal prediction**

While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the *uncertainty* of predictions. 
We will introduce a recent line of work called distribution-free predictive inference (a.k.a. *conformal prediction*) that give prediction intervals with finite-sample statistical guarantees for any (possibly incorrectly specified) predictive model and any (unknown) underlying distribution of the data, ensuring reliable uncertainty quantification for predictions.

[Course notes on conformal prediction](docs/conformal.pdf)

<table>

<tr>
<td>*Topic*</td>
<td>*R code*</td>
</tr>

<tr>
<td>Marginal vs conditional coverage</td>
<td>[coverage](docs/coverage.txt)</td>
</tr>

<tr>
<td>Heteroskedastic data</td>
<td>[heteroskedastic](docs/heteroskedastic.txt)</td>
</tr>

<tr>
<td>Split conformal prediction function</td>
<td>[cp.split](docs/cp.split.txt)</td>
</tr>

</table>


**Variable selection with statistical guarantees**

In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and we would like to be able to discover which predictors are important for the response. We will introduce a recent set of methods (*sample splitting*, *stability selection* and the *knockoff filter*) that allow to identify the truly important predictors with rigorous statistical guarantees.

[Course notes on variable selection](docs/vs.pdf)

<table>

<tr>
<td>*Topic*</td>
<td>*R code*</td>
</tr>

<tr>
<td>High-dimensional regression</td>
<td>[regression_hd](docs/regression_hd.txt)</td>
</tr>

<tr>
<td>Stability selection</td>
<td>[stability](docs/stability.txt)</td>
</tr>

<tr>
<td>Knockoffs filter</td>
<td>[knockoffs](docs/knockoffs.txt)</td>
</tr>

</table>


## Calendar

|| N || Date || Hours || 
|-|-|-|-|-|-|-|-|-|-|-|
||  || ||  ||  || ||
|| 1 || November 29, 2021 || 10:00 - 12:00, 14:00 - 16:00 (4 hours)   ||
|| 2 || November 30, 2021 || 10:00 - 12:00, 14:00 - 16:00 (4 hours)   ||

## Exam

December 14, 2021, h 14:00

### Data analysis

* Prediction intervals: This exercise provides both a training set $(x_i,y_i)$, $i=1,\ldots,n$ (test.txt) and a test set $x^*_i$, $i=1,\ldots,m$ (train.txt). The goal is to provide a prediction interval $[L_i,U_i]$ for each (unknown) $y^*_i$ of the test set. Your prediction intervals will be evaluated on both coverage (target: 90%) and average length.


   * Data set [PI_LOW_train](docs/PI_LOW_train.txt) and [PI_LOW_test](docs/PI_LOW_test.txt) low-dimensional (train $n=200$, test $m=1000$ and dimension $d=10$)
   * Data set [PI_HIGH_train](docs/PI_HIGH_train.txt) e [PI_HIGH_test](docs/data/PI_HIGH_test.txt) high-dimensional (train $n=100$, test $m=1000$ and dimension $d=100$)
   * Submission example: [TXT](docs/data/2575_PI_LOW.txt) and [PDF](docs/data/2575_PI_LOW_SUPPL.pdf). 

* The goal is to select the ``relevant'' predictors for each dataset. Data were generated as $y = X\beta + \varepsilon$
with $S = \{j: \beta_j \neq 0\}$ and $N = \{j: \beta_j = 0\}$. Your selections $\hat{S}$ will be evaluated on 
both True Positive Rate and False Discovery Rate.
   * Data set [VS_LOW](docs/data/VS_LOW.txt) a bassa dimensionalità ($n=40$ e $d= 19$)
   * Data set [VS_HIGH](docs/data/VS_HIGH.txt) ad elevata dimensionalità ($n=1000$ e $d= 1000$)
   * Submission example: [TXT](docs/data/2575_VS_HIGH.txt), [RMD](docs/data/2575_VS_HIGH.Rmd) and [PDF](docs/data/2575_VS_HIGH.pdf).
 


