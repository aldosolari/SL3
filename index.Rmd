---
title: "Statistical Learning - module III"
output: markdowntemplates::minimal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Program

**Conformal prediction**

While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the *uncertainty* of predictions. 
We will introduce a recent line of work called distribution-free predictive inference (a.k.a. *conformal prediction*) that give prediction intervals with finite-sample statistical guarantees for any (possibly incorrectly specified) predictive model and any (unknown) underlying distribution of the data, ensuring reliable uncertainty quantification for predictions.

[Course notes on conformal prediction](docs/conformal.pdf)

<table>

<tr>
<td>*Topic*</td>
<td>*R code*</td>
</tr>

<tr>
<td>Marginal vs conditional coverage</td>
<td>[coverage](docs/coverage.txt)</td>
</tr>

<tr>
<td>Heteroskedastic data</td>
<td>[heteroskedastic](docs/heteroskedastic.txt)</td>
</tr>

<tr>
<td>Split conformal prediction function</td>
<td>[cp.split](docs/cp.split.txt)</td>
</tr>

</table>


**Variable selection with statistical guarantees**

In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and we would like to be able to discover which predictors are important for the response. We will introduce a recent set of methods (*sample splitting*, *stability selection* and the *knockoff filter*) that allow to identify the truly important predictors with rigorous statistical guarantees.

[Course notes on variable selection](docs/vs.pdf)

<table>

<tr>
<td>*Topic*</td>
<td>*R code*</td>
</tr>

<tr>
<td>High-dimensional regression</td>
<td>[regression_hd](docs/regression_hd.txt)</td>
</tr>

<tr>
<td>Stability selection</td>
<td>[stability](docs/stability.txt)</td>
</tr>

<tr>
<td>Knockoffs filter</td>
<td>[knockoffs](docs/knockoffs.txt)</td>
</tr>

</table>


## Calendar

|| N || Date || Hours || Room ||
|-|-|-|-|-|-|-|-|-|-|-|-|
||  || ||  ||  || ||
|| 1 || November 29, 2021 || 10:00 - 12:00, 14:00 - 16:00 (4 hours)   || U7-2104 ||
|| 2 || November 30, 2021 || 10:00 - 12:00, 14:00 - 16:00 (4 hours)   || U7-2062 ||

## Exam

The exam consists in performing statistical analysis on three sets of data and presenting the results. Any programming language is allowed, but the results must be reproducible. The deadline for the submission (via email) is December 10, 2021, h 17:00. Results will be discussed on December 14, 2021, h 14:00 room U7-2104.

### Data analysis

* **Prediction intervals**: This exercise provides both a training set $(x_i,y_i)$, $i=1,\ldots,n$ (test.txt) and a test set $x^*_i$, $i=1,\ldots,m$ (train.txt). The goal is to provide a prediction interval $[l_i,u_i]$ for each (unknown) $y^*_i$ of the test set. Your prediction intervals will be evaluated by the average width $\bar{w} = (1/m)\sum_{i=1}^{m}(u_i - l_i)$
provided that the target 90% coverage is achieved, i.e. $\bar{c}=(1/m)\sum_{i=1}^{m}1\{y^*_i \in [l_i,u_i] \} \geq .9$. If $\bar{c}<.9$, then your intervals will be adjusted by $[l_i-a,u_i + a]$ with $a$ the smallest value such that $(1/m)\sum_{i=1}^{m}1\{y^*_i \in [l_i-a,u_i+a] \} \geq .9$. In this case, the average width will be adjusted by $\bar{w} + 4a$. 

   * Data set [PI_LOW_train](docs/PI_LOW_train.txt) and [PI_LOW_test](docs/PI_LOW_test.txt) low-dimensional (train $n=200$, test $m=2000$ and dimension $d=10$)
   * Data set [PI_HIGH_train](docs/PI_HIGH_train.txt) e [PI_HIGH_test](docs/PI_HIGH_test.txt) high-dimensional (train $n=100$, test $m=2000$ and dimension $d=200$)
   * Submission example: [TXT](docs/2575_PI_LOW.txt) and [PDF](docs/2575_PI_LOW_SUPPL.pdf). 

* **Variable selection**: the goal is to select the ``relevant'' predictors. Your selection will be evaluated by the Matthews correlation coefficient.

   * Data set [VS](docs/VS.csv) with $n=767$ and $d=201$
   * Submission example: [TXT](docs/2575_VS.txt) and [PDF](docs/2575_VS_SUPPL.pdf).
 


