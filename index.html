<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Statistical Learning - module III</title>

<style type="text/css"></style>




</head>

<body>
<div class="container">

<h1>Statistical Learning - module III</h1>



<div id="program" class="section level2">
<h2>Program</h2>
<p><strong>Conformal prediction</strong></p>
<p>While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the <em>uncertainty</em> of predictions. We will introduce a recent line of work called distribution-free predictive inference (a.k.a. <em>conformal prediction</em>) that give prediction intervals with finite-sample statistical guarantees for any (possibly incorrectly specified) predictive model and any (unknown) underlying distribution of the data, ensuring reliable uncertainty quantification for predictions.</p>
<p><a href="docs/conformal.pdf">Course notes on conformal prediction</a></p>
<table>
<tr>
<td>
<em>Topic</em>
</td>
<td>
<em>R code</em>
</td>
</tr>
<tr>
<td>
Marginal vs conditional coverage
</td>
<td>
<a href="docs/coverage.txt">coverage</a>
</td>
</tr>
<tr>
<td>
Heteroskedastic data
</td>
<td>
<a href="docs/heteroskedastic.txt">heteroskedastic</a>
</td>
</tr>
<tr>
<td>
Split conformal prediction function
</td>
<td>
<a href="docs/cp.split.txt">cp.split</a>
</td>
</tr>
</table>
<p><strong>Variable selection with statistical guarantees</strong></p>
<p>In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and we would like to be able to discover which predictors are important for the response. We will introduce a recent set of methods (<em>sample splitting</em>, <em>stability selection</em> and the <em>knockoff filter</em>) that allow to identify the truly important predictors with rigorous statistical guarantees.</p>
<p><a href="docs/vs.pdf">Course notes on variable selection</a></p>
<table>
<tr>
<td>
<em>Topic</em>
</td>
<td>
<em>R code</em>
</td>
</tr>
<tr>
<td>
High-dimensional regression
</td>
<td>
<a href="docs/regression_hd.txt">regression_hd</a>
</td>
</tr>
<tr>
<td>
Stability selection
</td>
<td>
<a href="docs/stability.txt">stability</a>
</td>
</tr>
<tr>
<td>
Knockoffs filter
</td>
<td>
<a href="docs/knockoffs.txt">knockoffs</a>
</td>
</tr>
</table>
</div>
<div id="calendar" class="section level2">
<h2>Calendar</h2>
<table>
<thead>
<tr class="header">
<th></th>
<th>N</th>
<th></th>
<th>Date</th>
<th></th>
<th>Hours</th>
<th></th>
<th>Room</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>1</td>
<td></td>
<td>November 29, 2021</td>
<td></td>
<td>10:00 - 12:00, 14:00 - 16:00 (4 hours)</td>
<td></td>
<td>U7-2104</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>2</td>
<td></td>
<td>November 30, 2021</td>
<td></td>
<td>10:00 - 12:00, 14:00 - 16:00 (4 hours)</td>
<td></td>
<td>U7-2062</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="exam" class="section level2">
<h2>Exam</h2>
<p>The exam consists in performing statistical analysis on three sets of data and presenting the results. Any programming language is allowed, but the results must be reproducible. The deadline for the submission (via email) is December 10, 2021, h 17:00. Results will be discussed on December 14, 2021, h 14:00 room U7-2104.</p>
<div id="data-analysis" class="section level3">
<h3>Data analysis</h3>
<p><a href="docs/results.html">Results</a></p>
<ul>
<li><p><strong>Prediction intervals</strong>: This exercise provides both a training set <span class="math inline">\((x_i,y_i)\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span> (test.txt) and a test set <span class="math inline">\(x^*_i\)</span>, <span class="math inline">\(i=1,\ldots,m\)</span> (train.txt). The goal is to provide a prediction interval <span class="math inline">\([l_i,u_i]\)</span> for each (unknown) <span class="math inline">\(y^*_i\)</span> of the test set. Your prediction intervals will be evaluated by the average width <span class="math inline">\(\bar{w} = (1/m)\sum_{i=1}^{m}(u_i - l_i)\)</span> provided that the target 90% coverage is achieved, i.e. <span class="math inline">\(\bar{c}=(1/m)\sum_{i=1}^{m}1\{y^*_i \in [l_i,u_i] \} \geq .9\)</span>. If <span class="math inline">\(\bar{c}&lt;.9\)</span>, then your intervals will be adjusted by <span class="math inline">\([l_i-a,u_i + a]\)</span> with <span class="math inline">\(a\)</span> the smallest value such that <span class="math inline">\((1/m)\sum_{i=1}^{m}1\{y^*_i \in [l_i-a,u_i+a] \} \geq .9\)</span>. In this case, the average width will be adjusted by <span class="math inline">\(\bar{w} + 4a\)</span>.</p>
<ul>
<li>Data set <a href="docs/PI_LOW_train.txt">PI_LOW_train</a> and <a href="docs/PI_LOW_test.txt">PI_LOW_test</a> low-dimensional (train <span class="math inline">\(n=200\)</span>, test <span class="math inline">\(m=2000\)</span> and dimension <span class="math inline">\(d=10\)</span>)</li>
<li>Data set <a href="docs/PI_HIGH_train.txt">PI_HIGH_train</a> e <a href="docs/PI_HIGH_test.txt">PI_HIGH_test</a> high-dimensional (train <span class="math inline">\(n=100\)</span>, test <span class="math inline">\(m=2000\)</span> and dimension <span class="math inline">\(d=200\)</span>)</li>
<li>Submission example: <a href="docs/2575_PI_LOW.txt">TXT</a> and <a href="docs/2575_PI_LOW_SUPPL.pdf">PDF</a>.</li>
</ul></li>
<li><p><strong>Variable selection</strong>: the goal is to select the ``relevant’’ predictors. Your selection will be evaluated by the Matthews correlation coefficient.</p>
<ul>
<li>Data set <a href="docs/VS.csv">VS</a> with <span class="math inline">\(n=767\)</span> and <span class="math inline">\(d=201\)</span></li>
<li>Submission example: <a href="docs/2575_VS.txt">TXT</a> and <a href="docs/2575_VS_SUPPL.pdf">PDF</a>.</li>
</ul></li>
</ul>
</div>
</div>


</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
